<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AR Astronave con SLAM</title>
  
  <!-- A-Frame for rendering the scene -->
  <script src="https://aframe.io/releases/1.2.0/aframe.min.js"></script>
  
  <!-- jsfeat library for feature detection and tracking -->
  <script src="https://cdn.jsdelivr.net/npm/jsfeat@0.0.8/build/jsfeat.min.js"></script>

  <style>
    body {
      margin: 0;
      overflow: hidden;
    }
    #video, #canvas {
      position: absolute;
      top: 0;
      left: 0;
      z-index: 0; /* Ensure canvas and video are behind the A-Frame scene */
    }
    a-scene {
      position: absolute;
      top: 0;
      left: 0;
      width: 100vw;
      height: 100vh;
      z-index: 1; /* Ensure the A-Frame scene is above the video and canvas */
    }
  </style>
</head>
<body>

<!-- A-Frame Scene -->
<a-scene embedded 
         renderer="colorManagement: true, physicallyCorrectLights" 
         vr-mode-ui="enabled: false" 
         device-orientation-permission-ui="enabled: false">
    
  <a-assets>
      <a-asset-item id="astro" src="ASTRONAVE.glb"></a-asset-item>
  </a-assets>

  <!-- Astronave Model -->
  <a-entity id="astronave" position="0 0 -5" gltf-model="#astro" scale="1 1 1"></a-entity>

  <!-- Debug Box -->
  <a-box position="0 0 -10" rotation="0 45 0" color="#4CC3D9"></a-box>

  <!-- Camera for AR scene -->
  <a-camera id="ar-camera" position="0 0 0" look-controls="enabled: false" 
            cursor="fuse: false; rayOrigin: mouse;"
            raycaster="objects: .clickable"></a-camera>
</a-scene>

<!-- Video Feed from Camera -->
<video id="video" width="640" height="480" autoplay></video>
<canvas id="canvas" width="640" height="480"></canvas>

<script type="text/javascript">
  const video = document.getElementById('video');
  const canvas = document.getElementById('canvas');
  const ctx = canvas.getContext('2d');
  const modelEntity = document.getElementById('astronave');
  
  // Initialize jsfeat variables
  const width = canvas.width;
  const height = canvas.height;
  const img_u8 = new jsfeat.matrix_t(width, height, jsfeat.U8_t | jsfeat.C1_t);
  const corners = [];
  for (let i = 0; i < width * height; i++) corners[i] = new jsfeat.keypoint_t(0, 0, 0, 0);

  navigator.mediaDevices.getUserMedia({ video: true })
    .then((stream) => {
      video.srcObject = stream;

      video.addEventListener('loadedmetadata', () => {
        console.log('Video metadata loaded. Video dimensions:', video.videoWidth, video.videoHeight);
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        video.play();
      });

      video.addEventListener('play', () => {
        console.log('Video playing');
        processVideo();
      });
    })
    .catch((error) => {
      console.error('Error accessing camera: ', error);
    });

  function detectFeatures() {
    console.log('Detecting features');

    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);

    // Convert to grayscale
    jsfeat.imgproc.grayscale(imageData.data, width, height, img_u8);

    // Detect keypoints
    const count = jsfeat.yape06.detect(img_u8, corners, 100);

    // Collect detected points
    const points = [];
    for (let i = 0; i < count; i++) {
      points.push({ x: corners[i].x, y: corners[i].y });
    }

    return points;
  }

  function estimateCameraPose(points) {
    console.log('Estimating camera pose');

    if (points.length > 0) {
      const avgX = points.reduce((sum, p) => sum + p.x, 0) / points.length;
      const avgY = points.reduce((sum, p) => sum + p.y, 0) / points.length;

      // Convert 2D screen position to 3D world position (adjust scaling as necessary)
      const x3D = (avgX / canvas.width) * 10 - 5;
      const y3D = (avgY / canvas.height) * 10 - 5;

      // Update the model's position in the scene
      modelEntity.setAttribute('position', `${x3D} ${y3D} -5`);
    }
  }

  function processVideo() {
    console.log('Processing video frame');

    const points = detectFeatures();
    console.log('Detected points:', points);

    if (points.length > 0) {
      estimateCameraPose(points);
    } else {
      console.log('No points detected.');
    }

    // Schedule next frame
    requestAnimationFrame(processVideo);
  }
</script>

</body>
</html>
