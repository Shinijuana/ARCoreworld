<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AR Astronave con SLAM</title>
  
  <!-- A-Frame for rendering the scene -->
  <script src="https://aframe.io/releases/1.2.0/aframe.min.js"></script>
  
  <!-- jsfeat for feature detection and tracking -->
  <script src="https://inspirit.github.io/jsfeat/build/jsfeat-min.js"></script>

</head>
<body style="margin: 0; overflow: hidden;">

<!-- A-Frame Scene -->
<a-scene embedded 
         renderer="colorManagement: true, physicallyCorrectLights" 
         vr-mode-ui="enabled: false" 
         device-orientation-permission-ui="enabled: false">
    
  <a-assets>
      <a-asset-item id="astro" src="ASTRONAVE.glb"></a-asset-item>
  </a-assets>

  <!-- Astronave Model -->
  <a-entity id="astronave" position="0 0 0" gltf-model="#astro" scale="1 1 1"></a-entity>

  <!-- Camera for AR scene -->
  <a-camera id="ar-camera" position="0 0 0" look-controls="enabled: false" 
            cursor="fuse: false; rayOrigin: mouse;"></a-camera>
</a-scene>

<!-- Video Feed from Camera -->
<video id="video" width="640" height="480" autoplay style="display: none;"></video>

<script type="text/javascript">
  const video = document.getElementById('video');
  const modelEntity = document.getElementById('astronave');

  // Start video stream
  navigator.mediaDevices.getUserMedia({ video: true }).then((stream) => {
    video.srcObject = stream;
    video.play();
    processVideo();
  });

  function processVideo() {
    const width = video.videoWidth;
    const height = video.videoHeight;
    
    // Create canvas for processing
    const canvas = document.createElement('canvas');
    canvas.width = width;
    canvas.height = height;
    const ctx = canvas.getContext('2d');
    
    // Setup jsfeat
    const img_u8 = new jsfeat.matrix_t(width, height, jsfeat.U8_t | jsfeat.C1_t);
    const corners = [];
    for (let i = 0; i < width * height; i++) corners[i] = new jsfeat.keypoint_t(0, 0, 0, 0);

    function detectFeatures() {
      ctx.drawImage(video, 0, 0, width, height);
      const imageData = ctx.getImageData(0, 0, width, height);

      // Convert to grayscale
      jsfeat.imgproc.grayscale(imageData.data, width, height, img_u8);

      // Detect keypoints using jsfeat
      const count = jsfeat.yape06.detect(img_u8, corners, 100);

      // Filter good points
      const points = [];
      for (let i = 0; i < count; i++) {
        points.push({ x: corners[i].x, y: corners[i].y });
      }

      return points;
    }

    function estimateCameraPose(points) {
      // If enough points, calculate an average to simulate movement
      if (points.length > 0) {
        const avgX = points.reduce((sum, p) => sum + p.x, 0) / points.length;
        const avgY = points.reduce((sum, p) => sum + p.y, 0) / points.length;

        // Convert 2D screen position to 3D world position (adjust scaling as necessary)
        const x3D = (avgX / width) * 10 - 5;
        const y3D = (avgY / height) * 10 - 5;

        // Update the model's position in the scene
        modelEntity.setAttribute('position', `${x3D} ${y3D} -5`);
      }
    }

    function track() {
      const points = detectFeatures();
      estimateCameraPose(points);
      requestAnimationFrame(track);
    }

    track();
  }
</script>

</body>
</html>
